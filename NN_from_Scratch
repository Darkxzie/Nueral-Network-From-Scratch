{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# MNSIT Nueral Network\n"
      ],
      "metadata": {
        "id": "Um2D0UEbCHeE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bgx_ozikh1DW"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.datasets import mnist\n",
        "\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "\n",
        "x_train = x_train.reshape(-1, 28*28).astype(np.float32) / 255.0\n",
        "x_test = x_test.reshape(-1, 28*28).astype(np.float32) / 255.0\n",
        "\n",
        "def one_hot_encode(y, num_classes=10):\n",
        "    encoded = np.zeros((y.shape[0], num_classes))\n",
        "    encoded[np.arange(y.shape[0]), y] = 1\n",
        "    return encoded\n",
        "\n",
        "y_train_oh = one_hot_encode(y_train)\n",
        "y_test_oh = one_hot_encode(y_test)\n",
        "\n",
        "print(f\"Train shape: {x_train.shape}, Test shape: {x_test.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def relu_derivative(x):\n",
        "    return (x > 0).astype(np.float32)\n",
        "\n",
        "def softmax(x):\n",
        "    exps = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
        "    return exps / np.sum(exps, axis=1, keepdims=True)\n",
        "def cross_entropy(pred, target):\n",
        "    pred = np.clip(pred, 1e-9, 1 - 1e-9)\n",
        "    return -np.mean(np.sum(target * np.log(pred), axis=1))\n",
        "\n",
        "def cross_entropy_derivative(pred, target):\n",
        "    return pred - target\n"
      ],
      "metadata": {
        "id": "UMjb2k94CPiK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralNetwork:\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        self.w1 = np.random.randn(input_size, hidden_size) * np.sqrt(2. / input_size)\n",
        "        self.b1 = np.zeros((1, hidden_size))\n",
        "        self.w2 = np.random.randn(hidden_size, output_size) * np.sqrt(2. / hidden_size)\n",
        "        self.b2 = np.zeros((1, output_size))\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.z1 = x @ self.w1 + self.b1\n",
        "        self.a1 = relu(self.z1)\n",
        "        self.z2 = self.a1 @ self.w2 + self.b2\n",
        "        self.a2 = softmax(self.z2)\n",
        "        return self.a2\n",
        "\n",
        "    def backward(self, x, y, output, lr=0.01):\n",
        "        m = y.shape[0]\n",
        "        dz2 = cross_entropy_derivative(output, y)\n",
        "        dw2 = self.a1.T @ dz2 / m\n",
        "        db2 = np.sum(dz2, axis=0, keepdims=True) / m\n",
        "\n",
        "        dz1 = (dz2 @ self.w2.T) * relu_derivative(self.z1)\n",
        "        dw1 = x.T @ dz1 / m\n",
        "        db1 = np.sum(dz1, axis=0, keepdims=True) / m\n",
        "\n",
        "        # Update weights\n",
        "        self.w2 -= lr * dw2\n",
        "        self.b2 -= lr * db2\n",
        "        self.w1 -= lr * dw1\n",
        "        self.b1 -= lr * db1\n",
        "def accuracy(predictions, labels):\n",
        "    return np.mean(np.argmax(predictions, axis=1) == np.argmax(labels, axis=1))\n",
        "\n",
        "def train(model, x_train, y_train, x_test, y_test, epochs=10, batch_size=64, lr=0.01):\n",
        "    for epoch in range(epochs):\n",
        "        indices = np.random.permutation(len(x_train))\n",
        "        x_train_shuffled = x_train[indices]\n",
        "        y_train_shuffled = y_train[indices]\n",
        "\n",
        "        for i in range(0, len(x_train), batch_size):\n",
        "            x_batch = x_train_shuffled[i:i+batch_size]\n",
        "            y_batch = y_train_shuffled[i:i+batch_size]\n",
        "            output = model.forward(x_batch)\n",
        "            model.backward(x_batch, y_batch, output, lr)\n",
        "\n",
        "        # Epoch evaluation\n",
        "        train_output = model.forward(x_train)\n",
        "        test_output = model.forward(x_test)\n",
        "        train_loss = cross_entropy(train_output, y_train)\n",
        "        test_loss = cross_entropy(test_output, y_test)\n",
        "        test_acc = accuracy(test_output, y_test)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs} | Train Loss: {train_loss:.4f} | Test Loss: {test_loss:.4f} | Test Acc: {test_acc*100:.2f}%\")\n",
        "\n",
        "model = NeuralNetwork(input_size=784, hidden_size=128, output_size=10)\n",
        "\n",
        "train(model, x_train, y_train_oh, x_test, y_test_oh, epochs=10, batch_size=64, lr=0.1)\n"
      ],
      "metadata": {
        "id": "Zt9AD1EODI8X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_weights(weights, num_rows=8, num_cols=8):\n",
        "    fig, axes = plt.subplots(num_rows, num_cols, figsize=(12, 12))\n",
        "    for i, ax in enumerate(axes.flat):\n",
        "        if i < weights.shape[1]:\n",
        "            img = weights[:, i].reshape(28, 28)\n",
        "            ax.imshow(img, cmap='viridis')\n",
        "            ax.axis('off')\n",
        "    plt.suptitle(\"First Layer Weights (as Images)\", fontsize=16)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "visualize_weights(model.w1)\n"
      ],
      "metadata": {
        "id": "ryLmSxZoDwlq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, x_train, y_train, x_test, y_test, epochs=100, batch_size=64, lr=0.01):\n",
        "    history = {\n",
        "        'train_loss': [],\n",
        "        'test_loss': [],\n",
        "        'test_accuracy': []\n",
        "    }\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        indices = np.random.permutation(len(x_train))\n",
        "        x_train_shuffled = x_train[indices]\n",
        "        y_train_shuffled = y_train[indices]\n",
        "\n",
        "        for i in range(0, len(x_train), batch_size):\n",
        "            x_batch = x_train_shuffled[i:i+batch_size]\n",
        "            y_batch = y_train_shuffled[i:i+batch_size]\n",
        "            output = model.forward(x_batch)\n",
        "            model.backward(x_batch, y_batch, output, lr)\n",
        "\n",
        "        # Evaluate at end of epoch\n",
        "        train_output = model.forward(x_train)\n",
        "        test_output = model.forward(x_test)\n",
        "\n",
        "        train_loss = cross_entropy(train_output, y_train)\n",
        "        test_loss = cross_entropy(test_output, y_test)\n",
        "        test_acc = accuracy(test_output, y_test)\n",
        "\n",
        "        history['train_loss'].append(train_loss)\n",
        "        history['test_loss'].append(test_loss)\n",
        "        history['test_accuracy'].append(test_acc)\n",
        "\n",
        "        # Print every 10 epochs\n",
        "        if (epoch + 1) % 10 == 0 or epoch == 0:\n",
        "            print(f\"Epoch {epoch+1}/{epochs} | \"\n",
        "                  f\"Train Loss: {train_loss:.4f} | \"\n",
        "                  f\"Test Loss: {test_loss:.4f} | \"\n",
        "                  f\"Test Acc: {test_acc * 100:.2f}%\")\n",
        "\n",
        "    return history\n",
        "\n",
        "# Run the training\n",
        "history = train(model, x_train, y_train_oh, x_test, y_test_oh, epochs=100, batch_size=64, lr=0.1)\n",
        "\n",
        "# Plot Loss and Accuracy Over 100 Epochs\n",
        "epochs_range = range(1, len(history['train_loss']) + 1)\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# Loss Plot\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs_range, history['train_loss'], label='Train Loss')\n",
        "plt.plot(epochs_range, history['test_loss'], label='Test Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Loss Over Epochs')\n",
        "plt.legend()\n",
        "\n",
        "# Accuracy Plot\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs_range, [a * 100 for a in history['test_accuracy']], label='Test Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.title('Test Accuracy Over Epochs')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ebnFC8dyFVp1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JBhM0p8mIQYi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
